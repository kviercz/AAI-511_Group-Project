{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObiHh9BXtyTp1w2/IOGT/G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kviercz/AAI-511_Group-Project/blob/main/Final_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eokU_uiirTy"
      },
      "outputs": [],
      "source": [
        "# This is just for my runtime, can comment out\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "dataset_path = '/content/drive/MyDrive/DataFiles/'\n",
        "os.chdir(dataset_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pretty_midi mido\n",
        "\n",
        "import pretty_midi\n",
        "import librosa\n",
        "import glob\n",
        "import random\n",
        "import tesnorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding, Bidirectional\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from mido import KeySignatureError, MidiFile\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "ekFgVeupi9sF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data processing and initial loading\n",
        "The following two functions are helper functions that help load in the data from the different folders (Bach, Chopin, Beethoven, and Motzart). They extract tempo and note sequences for each file, as well as grab all the features and the outer folder label."
      ],
      "metadata": {
        "id": "A6e_uyIqkteC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_midi_features(file_path):\n",
        "  \"\"\"\n",
        "  Extract tempo and note sequences from a MIDI file.\n",
        "\n",
        "  Parameters:\n",
        "      file_path (str): Path to the MIDI file.\n",
        "\n",
        "  Returns:\n",
        "      tuple: A tuple containing the tempo and note sequences.\n",
        "  \"\"\"\n",
        "    midi_data = pretty_midi.PrettyMIDI(file_path)\n",
        "\n",
        "    # Extract tempo from the midi file\n",
        "    tempo = midi_data.estimate_tempo()\n",
        "\n",
        "    # Extract note sequences\n",
        "    note_sequences = []\n",
        "    for instrument in midi_data.instruments:\n",
        "        for note in instrument.notes:\n",
        "            note_sequences.append([note.start, note.end, note.pitch, note.velocity])\n",
        "\n",
        "    return tempo, note_sequences\n",
        "\n",
        "def process_composer_data(composer_path):\n",
        "  \"\"\"\n",
        "  Process MIDI files for a specific composer and extract features and labels.\n",
        "\n",
        "  Parameters:\n",
        "      composer_path (str): Path to the directory containing MIDI files for a specific composer.\n",
        "\n",
        "  Returns:\n",
        "      tuple: A tuple containing two lists - features and labels.\n",
        "  \"\"\"\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    for midi_file in glob.glob(os.path.join(composer_path, '*.mid')):\n",
        "        try:\n",
        "            tempo, note_sequences = extract_midi_features(midi_file)\n",
        "            features.append((tempo, note_sequences))\n",
        "            labels.append(composer_path.split('/')[-1])\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {midi_file}: {e}\")\n",
        "\n",
        "    return features, labels\n",
        "\n"
      ],
      "metadata": {
        "id": "vfC0OlvRjWh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def analyze_basic_midi_statistics(midi_file_path):\n",
        "  try:\n",
        "    midi = MidiFile(midi_file_path)\n",
        "  except Exception as e:\n",
        "    print(f\"Error processing {midi_file_path}: {e}\")\n",
        "    return None\n",
        "\n",
        "  midi = MidiFile(midi_file_path)\n",
        "\n",
        "  num_tracks = len(midi.tracks)\n",
        "  total_ticks = midi.length\n",
        "  note_count = 0\n",
        "  instruments = set()\n",
        "\n",
        "  for track in midi.tracks:\n",
        "      for msg in track:\n",
        "          if msg.type == 'note_on':\n",
        "              note_count += 1\n",
        "              instruments.add(msg.channel)\n",
        "\n",
        "  statistics = {\n",
        "      'num_tracks': num_tracks,\n",
        "      'total_length_ticks': total_ticks,\n",
        "      'total_notes': note_count,\n",
        "      'unique_instruments': len(instruments),\n",
        "    }\n",
        "\n",
        "  return statistics\n",
        "\n",
        "def analyze_note_distribution(midi_file_path):\n",
        "    midi = MidiFile(midi_file_path)\n",
        "    note_counts = Counter()\n",
        "\n",
        "    for track in midi.tracks:\n",
        "        for msg in track:\n",
        "            if msg.type == 'note_on' and msg.velocity > 0:\n",
        "                note_counts[msg.note] += 1\n",
        "\n",
        "    return note_counts\n",
        "\n",
        "def analyze_rhythmic_patterns(midi_file_path):\n",
        "    midi = MidiFile(midi_file_path)\n",
        "\n",
        "    durations = []\n",
        "    tempo_changes = []\n",
        "    time_signature_changes = []\n",
        "\n",
        "    current_time = 0\n",
        "    last_note_on_time = {}\n",
        "    tempo = 500000  # Default tempo (microseconds per beat)\n",
        "\n",
        "    for track in midi.tracks:\n",
        "        for msg in track:\n",
        "            current_time += msg.time\n",
        "            if msg.type == 'set_tempo':\n",
        "                tempo = msg.tempo\n",
        "                tempo_changes.append((current_time, tempo))\n",
        "            elif msg.type == 'time_signature':\n",
        "                time_signature_changes.append((current_time, (msg.numerator, msg.denominator)))\n",
        "            elif msg.type == 'note_on' and msg.velocity > 0:\n",
        "                last_note_on_time[msg.note] = current_time\n",
        "            elif msg.type == 'note_off' or (msg.type == 'note_on' and msg.velocity == 0):\n",
        "                if msg.note in last_note_on_time:\n",
        "                    duration = current_time - last_note_on_time[msg.note]\n",
        "                    durations.append(duration)\n",
        "                    del last_note_on_time[msg.note]\n",
        "\n",
        "    avg_duration = np.mean(durations) if durations else 0\n",
        "    rhythmic_patterns = {\n",
        "        'average_note_duration': avg_duration,\n",
        "        'tempo_changes': tempo_changes,\n",
        "        'time_signature_changes': time_signature_changes\n",
        "    }\n",
        "\n",
        "    return rhythmic_patterns\n",
        "\n",
        "def analyze_instrument_distribution(midi_file_path):\n",
        "    midi = MidiFile(midi_file_path)\n",
        "    instrument_counts = Counter()\n",
        "\n",
        "    for track in midi.tracks:\n",
        "        for msg in track:\n",
        "            if msg.type == 'program_change':\n",
        "                instrument_counts[msg.program] += 1\n",
        "\n",
        "    return instrument_counts\n",
        "\n",
        "def analyze_velocity_dynamics(midi_file_path):\n",
        "    midi = MidiFile(midi_file_path)\n",
        "    velocities = []\n",
        "\n",
        "    for track in midi.tracks:\n",
        "        for msg in track:\n",
        "            if msg.type == 'note_on' and msg.velocity > 0:\n",
        "                velocities.append(msg.velocity)\n",
        "\n",
        "    avg_velocity = np.mean(velocities) if velocities else 0\n",
        "    max_velocity = np.max(velocities) if velocities else 0\n",
        "    min_velocity = np.min(velocities) if velocities else 0\n",
        "\n",
        "    dynamics = {\n",
        "        'average_velocity': avg_velocity,\n",
        "        'max_velocity': max_velocity,\n",
        "        'min_velocity': min_velocity,\n",
        "        'velocity_distribution': Counter(velocities)\n",
        "    }\n",
        "\n",
        "    return dynamics\n",
        "\n",
        "\n",
        "def process_midi_dataset(dataset_path):\n",
        "    \"\"\"\n",
        "    Process a dataset of MIDI files and aggregate the analysis results.\n",
        "\n",
        "    Parameters:\n",
        "        dataset_path (str): Path to the directory containing MIDI files.\n",
        "\n",
        "    Returns:\n",
        "        dict: Aggregated statistics and insights for the entire dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    basic_stats = {\n",
        "        'total_tracks': 0,\n",
        "        'total_length_ticks': 0,\n",
        "        'total_notes': 0,\n",
        "        'unique_instruments': Counter()\n",
        "    }\n",
        "    note_distribution = Counter()\n",
        "    rhythmic_patterns = {\n",
        "        'average_note_durations': [],\n",
        "        'tempo_changes': [],\n",
        "        'time_signature_changes': Counter()\n",
        "    }\n",
        "    instrument_distribution = Counter()\n",
        "    velocity_dynamics = {\n",
        "        'average_velocities': [],\n",
        "        'max_velocities': [],\n",
        "        'min_velocities': [],\n",
        "        'velocity_distribution': Counter()\n",
        "    }\n",
        "\n",
        "    # Iterate over each MIDI file in the dataset\n",
        "    for root, _, files in os.walk(dataset_path):\n",
        "        for file in files:\n",
        "            if file.endswith('.mid'):\n",
        "                midi_file_path = os.path.join(root, file)\n",
        "\n",
        "                # Analyze basic statistics\n",
        "                stats = analyze_basic_midi_statistics(midi_file_path)\n",
        "                if stats is None:\n",
        "                  continue\n",
        "\n",
        "\n",
        "                basic_stats['total_tracks'] += stats['num_tracks']\n",
        "                basic_stats['total_length_ticks'] += stats['total_length_ticks']\n",
        "                basic_stats['total_notes'] += stats['total_notes']\n",
        "                basic_stats['unique_instruments'].update([stats['unique_instruments']])\n",
        "\n",
        "                # Analyze note distribution\n",
        "                note_counts = analyze_note_distribution(midi_file_path)\n",
        "                note_distribution.update(note_counts)\n",
        "\n",
        "                # Analyze rhythmic patterns\n",
        "                rhythm_analysis = analyze_rhythmic_patterns(midi_file_path)\n",
        "                rhythmic_patterns['average_note_durations'].append(rhythm_analysis['average_note_duration'])\n",
        "                rhythmic_patterns['tempo_changes'].extend(rhythm_analysis['tempo_changes'])\n",
        "                rhythmic_patterns['time_signature_changes'].update(rhythm_analysis['time_signature_changes'])\n",
        "\n",
        "                # Analyze instrument distribution\n",
        "                instrument_counts = analyze_instrument_distribution(midi_file_path)\n",
        "                instrument_distribution.update(instrument_counts)\n",
        "\n",
        "                # Analyze velocity dynamics\n",
        "                dynamics = analyze_velocity_dynamics(midi_file_path)\n",
        "                velocity_dynamics['average_velocities'].append(dynamics['average_velocity'])\n",
        "                velocity_dynamics['max_velocities'].append(dynamics['max_velocity'])\n",
        "                velocity_dynamics['min_velocities'].append(dynamics['min_velocity'])\n",
        "                velocity_dynamics['velocity_distribution'].update(dynamics['velocity_distribution'])\n",
        "\n",
        "    # Aggregate results\n",
        "    aggregated_results = {\n",
        "        'basic_stats': {\n",
        "            'total_tracks': basic_stats['total_tracks'],\n",
        "            'total_length_ticks': basic_stats['total_length_ticks'],\n",
        "            'total_notes': basic_stats['total_notes'],\n",
        "            'unique_instruments_count': len(basic_stats['unique_instruments']),\n",
        "            'average_instruments_per_file': np.mean(list(basic_stats['unique_instruments'].elements()))\n",
        "        },\n",
        "        'note_distribution': note_distribution,\n",
        "        'rhythmic_patterns': {\n",
        "            'average_note_duration': np.mean(rhythmic_patterns['average_note_durations']),\n",
        "            'total_tempo_changes': len(rhythmic_patterns['tempo_changes']),\n",
        "            'common_time_signatures': rhythmic_patterns['time_signature_changes'].most_common(3)\n",
        "        },\n",
        "        'instrument_distribution': instrument_distribution,\n",
        "        'velocity_dynamics': {\n",
        "            'average_velocity': np.mean(velocity_dynamics['average_velocities']) if velocity_dynamics['average_velocities'] else 0,  # Handle empty list\n",
        "            'max_velocity': np.max(velocity_dynamics['max_velocities']) if velocity_dynamics['max_velocities'] else 0,  # Handle empty list\n",
        "            'min_velocity': np.min(velocity_dynamics['min_velocities']) if velocity_dynamics['min_velocities'] else 0,  # Handle empty list\n",
        "            'common_velocities': velocity_dynamics['velocity_distribution'].most_common(3)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return aggregated_results\n",
        "\n",
        "def plot_aggregated_results(aggregated_results):\n",
        "    \"\"\"\n",
        "    Plot the aggregated results from the MIDI dataset analysis.\n",
        "\n",
        "    Parameters:\n",
        "        aggregated_results (dict): Aggregated statistics and insights for the entire dataset.\n",
        "    \"\"\"\n",
        "    # Plot Note Distribution\n",
        "    notes = list(aggregated_results['note_distribution'].keys())\n",
        "    counts = list(aggregated_results['note_distribution'].values())\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.bar(notes, counts, color='skyblue')\n",
        "    plt.xlabel('MIDI Note Number')\n",
        "    plt.ylabel('Count')\n",
        "    plt.title('Overall Note Distribution')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot Instrument Distribution\n",
        "    instruments = list(aggregated_results['instrument_distribution'].keys())\n",
        "    counts = list(aggregated_results['instrument_distribution'].values())\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.bar(instruments, counts, color='lightgreen')\n",
        "    plt.xlabel('MIDI Instrument Number')\n",
        "    plt.ylabel('Count')\n",
        "    plt.title('Overall Instrument Distribution')\n",
        "    plt.show()\n",
        "\n",
        "def process_and_plot_all_folders(parent_dir):\n",
        "    \"\"\"\n",
        "    Process all subfolders within a parent directory,\n",
        "    analyze MIDI files in each, and plot the aggregated results.\n",
        "    \"\"\"\n",
        "    for folder_name in os.listdir(parent_dir):\n",
        "        folder_path = os.path.join(parent_dir, folder_name)\n",
        "        if os.path.isdir(folder_path):\n",
        "            print(f\"Processing folder: {folder_name}\")\n",
        "            aggregated_results = process_midi_dataset(folder_path)\n",
        "            print(aggregated_results)\n",
        "            plot_aggregated_results(aggregated_results)\n",
        "\n",
        "\n",
        "# Example Usage\n",
        "dataset_path = '/content/drive/MyDrive/DataFiles/'\n",
        "# process_and_plot_all_folders(dataset_path)\n",
        "# aggregated_results = process_midi_dataset(dataset_path)\n",
        "# print(aggregated_results)\n",
        "# plot_aggregated_results(aggregated_results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmXL5HLMZ0xL",
        "outputId": "c18ad75a-002f-4b98-f96e-351a7a54c435"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mido in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: packaging~=23.1 in /usr/local/lib/python3.10/dist-packages (from mido) (23.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA\n",
        "The following sections of code preform EDA on the data to help determine which features are helpful as well as visualizing the differences in the composers as far as instrument composition, quantity of data, and stats given the different features."
      ],
      "metadata": {
        "id": "wg7hKFlJlQgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic Statistics\n",
        "for folder_name in os.listdir(dataset_path):\n",
        "    folder_path = os.path.join(dataset_path, folder_name)\n",
        "    if os.path.isdir(folder_path):\n",
        "        print(f\"\\n--- Analyzing {folder_name} ---\")\n",
        "        aggregated_results = process_midi_dataset(folder_path)\n",
        "        print(\"Basic Statistics:\")\n",
        "        for key, value in aggregated_results['basic_stats'].items():\n",
        "            print(f\"  {key}: {value}\")\n",
        "\n",
        "# Rhythmic Patterns\n",
        "for folder_name in os.listdir(dataset_path):\n",
        "    folder_path = os.path.join(dataset_path, folder_name)\n",
        "    if os.path.isdir(folder_path):\n",
        "        print(f\"\\n--- Analyzing {folder_name} ---\")\n",
        "        aggregated_results = process_midi_dataset(folder_path)\n",
        "        print(\"Rhythmic Patterns:\")\n",
        "        print(f\"  Average Note Duration: {aggregated_results['rhythmic_patterns']['average_note_duration']}\")\n",
        "        print(f\"  Total Tempo Changes: {aggregated_results['rhythmic_patterns']['total_tempo_changes']}\")\n",
        "        print(\"  Common Time Signatures:\")\n",
        "        for signature, count in aggregated_results['rhythmic_patterns']['common_time_signatures']:\n",
        "            print(f\"    {signature}: {count}\")\n",
        "\n",
        "# Velocity Dynamics\n",
        "for folder_name in os.listdir(dataset_path):\n",
        "    folder_path = os.path.join(dataset_path, folder_name)\n",
        "    if os.path.isdir(folder_path):\n",
        "        print(f\"\\n--- Analyzing {folder_name} ---\")\n",
        "        aggregated_results = process_midi_dataset(folder_path)\n",
        "        print(\"Velocity Dynamics:\")\n",
        "        print(f\"  Average Velocity: {aggregated_results['velocity_dynamics']['average_velocity']}\")\n",
        "        print(f\"  Max Velocity: {aggregated_results['velocity_dynamics']['max_velocity']}\")\n",
        "        print(f\"  Min Velocity: {aggregated_results['velocity_dynamics']['min_velocity']}\")\n",
        "        print(\"  Common Velocities:\")\n",
        "        for velocity, count in aggregated_results['velocity_dynamics']['common_velocities']:\n",
        "            print(f\"    {velocity}: {count}\")\n"
      ],
      "metadata": {
        "id": "IdgVMu7hdsL_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b3ecc8e-d311-44ff-8e31-cf7c89a4f9a1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Analyzing Mozart ---\n",
            "Error processing /content/drive/MyDrive/DataFiles/Mozart/Piano Sonatas/Nueva carpeta/K281 Piano Sonata n03 3mov.mid: Could not decode key with 2 flats and mode 2\n",
            "Basic Statistics:\n",
            "  total_tracks: 2843\n",
            "  total_length_ticks: 102645.93818738835\n",
            "  total_notes: 2406373\n",
            "  unique_instruments_count: 15\n",
            "  average_instruments_per_file: 6.48828125\n",
            "\n",
            "--- Analyzing Beethoven ---\n",
            "Error processing /content/drive/MyDrive/DataFiles/Beethoven/Anhang 14-3.mid: Could not decode key with 3 flats and mode 255\n",
            "Basic Statistics:\n",
            "  total_tracks: 1904\n",
            "  total_length_ticks: 107564.80470104773\n",
            "  total_notes: 2710371\n",
            "  unique_instruments_count: 15\n",
            "  average_instruments_per_file: 5.502369668246446\n",
            "\n",
            "--- Analyzing Chopin ---\n",
            "Basic Statistics:\n",
            "  total_tracks: 984\n",
            "  total_length_ticks: 30084.67425522962\n",
            "  total_notes: 577457\n",
            "  unique_instruments_count: 8\n",
            "  average_instruments_per_file: 1.75\n",
            "\n",
            "--- Analyzing Bach ---\n",
            "Basic Statistics:\n",
            "  total_tracks: 6347\n",
            "  total_length_ticks: 141267.42845187857\n",
            "  total_notes: 2413465\n",
            "  unique_instruments_count: 15\n",
            "  average_instruments_per_file: 4.513513513513513\n",
            "\n",
            "--- Analyzing Mozart ---\n",
            "Error processing /content/drive/MyDrive/DataFiles/Mozart/Piano Sonatas/Nueva carpeta/K281 Piano Sonata n03 3mov.mid: Could not decode key with 2 flats and mode 2\n",
            "Rhythmic Patterns:\n",
            "  Average Note Duration: 154.16814529988562\n",
            "  Total Tempo Changes: 50201\n",
            "  Common Time Signatures:\n",
            "    (0, (4, 4)): 107\n",
            "    (0, (3, 4)): 66\n",
            "    (0, (2, 4)): 41\n",
            "\n",
            "--- Analyzing Beethoven ---\n",
            "Error processing /content/drive/MyDrive/DataFiles/Beethoven/Anhang 14-3.mid: Could not decode key with 3 flats and mode 255\n",
            "Rhythmic Patterns:\n",
            "  Average Note Duration: 151.941871306076\n",
            "  Total Tempo Changes: 98209\n",
            "  Common Time Signatures:\n",
            "    (0, (4, 4)): 66\n",
            "    (0, (3, 4)): 54\n",
            "    (0, (2, 4)): 49\n",
            "\n",
            "--- Analyzing Chopin ---\n",
            "Rhythmic Patterns:\n",
            "  Average Note Duration: 196.19499200958035\n",
            "  Total Tempo Changes: 60078\n",
            "  Common Time Signatures:\n",
            "    (0, (4, 4)): 65\n",
            "    (0, (3, 4)): 33\n",
            "    (0, (6, 8)): 13\n",
            "\n",
            "--- Analyzing Bach ---\n",
            "Rhythmic Patterns:\n",
            "  Average Note Duration: 588.0900457570657\n",
            "  Total Tempo Changes: 17973\n",
            "  Common Time Signatures:\n",
            "    (0, (4, 4)): 603\n",
            "    (0, (3, 4)): 133\n",
            "    (0, (3, 8)): 32\n",
            "\n",
            "--- Analyzing Mozart ---\n",
            "Error processing /content/drive/MyDrive/DataFiles/Mozart/Piano Sonatas/Nueva carpeta/K281 Piano Sonata n03 3mov.mid: Could not decode key with 2 flats and mode 2\n",
            "Velocity Dynamics:\n",
            "  Average Velocity: 78.11665526582848\n",
            "  Max Velocity: 127\n",
            "  Min Velocity: 1\n",
            "  Common Velocities:\n",
            "    105: 149993\n",
            "    80: 90865\n",
            "    92: 67825\n",
            "\n",
            "--- Analyzing Beethoven ---\n",
            "Error processing /content/drive/MyDrive/DataFiles/Beethoven/Anhang 14-3.mid: Could not decode key with 3 flats and mode 255\n",
            "Velocity Dynamics:\n",
            "  Average Velocity: 72.43653391720528\n",
            "  Max Velocity: 127\n",
            "  Min Velocity: 1\n",
            "  Common Velocities:\n",
            "    100: 122493\n",
            "    127: 118553\n",
            "    45: 52094\n",
            "\n",
            "--- Analyzing Chopin ---\n",
            "Velocity Dynamics:\n",
            "  Average Velocity: 66.10959120036694\n",
            "  Max Velocity: 127\n",
            "  Min Velocity: 1\n",
            "  Common Velocities:\n",
            "    100: 16669\n",
            "    60: 9703\n",
            "    70: 7648\n",
            "\n",
            "--- Analyzing Bach ---\n",
            "Velocity Dynamics:\n",
            "  Average Velocity: 91.55126380691169\n",
            "  Max Velocity: 127\n",
            "  Min Velocity: 1\n",
            "  Common Velocities:\n",
            "    96: 329827\n",
            "    100: 225878\n",
            "    76: 153365\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instrument Distribution\n",
        "The following code looks at how many times each instrument is used across the different composers, helps get a sense of the importance of instruments as a feature, as well as the distribution of the instruments each composer uses."
      ],
      "metadata": {
        "id": "jwoWhXeUnzWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pretty_midi import instrument_name_to_program, program_to_instrument_name\n",
        "\n",
        "\n",
        "for folder_name in os.listdir(dataset_path):\n",
        "    folder_path = os.path.join(dataset_path, folder_name)\n",
        "    if os.path.isdir(folder_path):\n",
        "        print(f\"\\n--- Instruments used by {folder_name} ---\")\n",
        "        aggregated_results = process_midi_dataset(folder_path)\n",
        "        for instrument, count in aggregated_results['instrument_distribution'].items():\n",
        "            instrument_name = program_to_instrument_name(instrument)\n",
        "            print(f\"  Instrument {instrument_name}: Used {count} times\")"
      ],
      "metadata": {
        "id": "ENgwHHolnvwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing\n",
        "The following section is dedicated to processing the data as well as extracting features that will be used in training of the model. Some key components are including windowing, data augmentation and the feature selection itself. Windowing allows the LSTM model to preform much better as it takes the data in sequences to learn based on those and ensures there is overlap in the windows, so the entire piece is learned, but the groups capture the features instead of note by note. Data augmentation is helpful here as well, as it ensures the model can be generalized and not a single composer is being learned in favor over the others. This helps reduce overfitting."
      ],
      "metadata": {
        "id": "PddOrz2AoBwU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load Data\n",
        "This initial function loads in the data from each composer, it returns the pitch, duration and velocity features for each processed file. There are a few corrupt files that do not fit the formatting that are caught and ignored with the exception. These features will be used in the rest of the data preprocessing steps."
      ],
      "metadata": {
        "id": "9stINAkArPD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_midi_features(directory):\n",
        "    \"\"\"Loads MIDI files and extracts pitch, duration, and velocity features.\"\"\"\n",
        "    pitch_sequences = []\n",
        "    duration_sequences = []\n",
        "    velocity_sequences = []\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".mid\"):\n",
        "            midi_path = os.path.join(directory, filename)\n",
        "            try:\n",
        "                midi = pretty_midi.PrettyMIDI(midi_path)\n",
        "                for instrument in midi.instruments:\n",
        "                    pitches = [note.pitch for note in instrument.notes]\n",
        "                    durations = [note.end - note.start for note in instrument.notes]\n",
        "                    velocities = [note.velocity for note in instrument.notes]\n",
        "                    pitch_sequences.append(pitches)\n",
        "                    duration_sequences.append(durations)\n",
        "                    velocity_sequences.append(velocities)\n",
        "            except KeySignatureError as e:\n",
        "                print(f\"Skipping file {filename} due to KeySignatureError: {e}\")\n",
        "\n",
        "    return pitch_sequences, duration_sequences, velocity_sequences\n",
        "\n",
        "# Load features for each composer\n",
        "bach_pitches, bach_durations, bach_velocities = load_midi_features(\"/content/drive/MyDrive/DataFiles/Bach/\")\n",
        "mozart_pitches, mozart_durations, mozart_velocities = load_midi_features(\"/content/drive/MyDrive/DataFiles/Mozart/\")\n",
        "beethoven_pitches, beethoven_durations, beethoven_velocities = load_midi_features(\"/content/drive/MyDrive/DataFiles/Beethoven/\")\n",
        "chopin_pitches, chopin_durations, chopin_velocities = load_midi_features(\"/content/drive/MyDrive/DataFiles/Chopin/\")\n"
      ],
      "metadata": {
        "id": "aFLpBUy0n_P0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalize Features\n",
        "The following function normalizes the features. This scales the values between 0 and 1 which will be helpful for the LSTM model to learn the correct weights to apply to each of the features by improving the gradient descent and helping the model converge faster since the scale is more \"normalized.\""
      ],
      "metadata": {
        "id": "9OI-XpP6rgde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_features(pitch_sequences, duration_sequences, velocity_sequences):\n",
        "    \"\"\"Normalizes pitches, durations, and velocities to a 0-1 range.\"\"\"\n",
        "    normalized_pitches = [[note / 127.0 for note in sequence] for sequence in pitch_sequences]\n",
        "    max_duration = max([max(durations) for durations in duration_sequences if durations])  # Find max duration\n",
        "    normalized_durations = [[duration / max_duration for duration in sequence] for sequence in duration_sequences]\n",
        "    normalized_velocities = [[velocity / 127.0 for velocity in sequence] for sequence in velocity_sequences]\n",
        "\n",
        "    return normalized_pitches, normalized_durations, normalized_velocities\n",
        "\n",
        "# Normalize the features\n",
        "bach_pitches, bach_durations, bach_velocities = normalize_features(bach_pitches, bach_durations, bach_velocities)\n",
        "mozart_pitches, mozart_durations, mozart_velocities = normalize_features(mozart_pitches, mozart_durations, mozart_velocities)\n",
        "beethoven_pitches, beethoven_durations, beethoven_velocities = normalize_features(beethoven_pitches, beethoven_durations, beethoven_velocities)\n",
        "chopin_pitches, chopin_durations, chopin_velocities = normalize_features(chopin_pitches, chopin_durations, chopin_velocities)"
      ],
      "metadata": {
        "id": "7bKhxGNVqyY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Augmentation\n",
        "The following functions augment and combine the features that can be passed on to the remainder of the data preprocessing stages. Augmentation helps prevent overfitting as well as allowing the model to better generalize the data. Given the imbalance in data, it also helps ensure that Bach is not just dominating, as it helps artificially generate data for the other composers based on the existing files."
      ],
      "metadata": {
        "id": "IbjJRIiUr54Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_sequence(sequence, num_augmentations=2):\n",
        "    \"\"\"Augments a note sequence by transposing and creating variations.\"\"\"\n",
        "    augmented_sequences = [sequence]\n",
        "    for _ in range(num_augmentations):\n",
        "        transpose_amount = random.randint(-5, 5)\n",
        "        augmented_sequence = [[note[0] + transpose_amount, note[1], note[2]] for note in sequence]\n",
        "        augmented_sequences.append(augmented_sequence)\n",
        "    return augmented_sequences\n",
        "\n",
        "def combine_features(pitch_sequences, duration_sequences, velocity_sequences):\n",
        "    \"\"\"Combines pitch, duration, and velocity features into a single array.\"\"\"\n",
        "    combined_features = []\n",
        "    for pitches, durations, velocities in zip(pitch_sequences, duration_sequences, velocity_sequences):\n",
        "        sequence_length = min(len(pitches), len(durations), len(velocities))\n",
        "        combined_sequence = []\n",
        "        for i in range(sequence_length):\n",
        "            combined_sequence.append([pitches[i], durations[i], velocities[i]])\n",
        "        combined_features.append(combined_sequence)\n",
        "    return combined_features\n",
        "\n",
        "# Combine features for each composer\n",
        "combined_bach = combine_features(bach_pitches, bach_durations, bach_velocities)\n",
        "combined_mozart = combine_features(mozart_pitches, mozart_durations, mozart_velocities)\n",
        "combined_beethoven = combine_features(beethoven_pitches, beethoven_durations, beethoven_velocities)\n",
        "combined_chopin = combine_features(chopin_pitches, chopin_durations, chopin_velocities)\n",
        "\n",
        "# Augment combined data\n",
        "augmented_combined_bach = [seq for orig_seq in combined_bach for seq in augment_sequence(orig_seq)]\n",
        "augmented_combined_mozart = [seq for orig_seq in combined_mozart for seq in augment_sequence(orig_seq)]\n",
        "augmented_combined_beethoven = [seq for orig_seq in combined_beethoven for seq in augment_sequence(orig_seq)]\n",
        "augmented_combined_chopin = [seq for orig_seq in combined_chopin for seq in augment_sequence(orig_seq)]"
      ],
      "metadata": {
        "id": "I7R2F5r4qzKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### More Augmentation\n",
        "The next two functions apply more feature augmentation specifically focused on noise and tempo, respectfully."
      ],
      "metadata": {
        "id": "wakh8ujIsTOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_with_noise(sequence, noise_factor=0.01):\n",
        "    \"\"\"Adds noise to the sequence to create variation.\"\"\"\n",
        "    noisy_sequence = []\n",
        "    for note in sequence:\n",
        "        noisy_note = [\n",
        "            note[0] + random.uniform(-noise_factor, noise_factor),\n",
        "            note[1] + random.uniform(-noise_factor, noise_factor),\n",
        "            note[2] + random.uniform(-noise_factor, noise_factor)\n",
        "        ]\n",
        "        noisy_sequence.append(noisy_note)\n",
        "    return noisy_sequence\n",
        "\n",
        "def augment_with_tempo(sequence, tempo_factor_range=(0.9, 1.1)):\n",
        "    \"\"\"Scales the tempo of the sequence.\"\"\"\n",
        "    tempo_factor = random.uniform(*tempo_factor_range)\n",
        "    tempo_scaled_sequence = [[note[0], note[1] * tempo_factor, note[2]] for note in sequence]\n",
        "    return tempo_scaled_sequence\n",
        "\n",
        "# Augment with noise and tempo scaling\n",
        "augmented_combined_bach.extend([augment_with_noise(seq) for seq in augmented_combined_bach])\n",
        "augmented_combined_bach.extend([augment_with_tempo(seq) for seq in augmented_combined_bach])\n",
        "\n",
        "augmented_combined_mozart.extend([augment_with_noise(seq) for seq in augmented_combined_mozart])\n",
        "augmented_combined_mozart.extend([augment_with_tempo(seq) for seq in augmented_combined_mozart])\n",
        "\n",
        "augmented_combined_beethoven.extend([augment_with_noise(seq) for seq in augmented_combined_beethoven])\n",
        "augmented_combined_beethoven.extend([augment_with_tempo(seq) for seq in augmented_combined_beethoven])\n",
        "\n",
        "augmented_combined_chopin.extend([augment_with_noise(seq) for seq in augmented_combined_chopin])\n",
        "augmented_combined_chopin.extend([augment_with_tempo(seq) for seq in augmented_combined_chopin])"
      ],
      "metadata": {
        "id": "W0QLQtWFq5zH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Windowing\n",
        "The final function applies windowing to the data. In previous training iterations, the model struggled to get past 33% accuracy, regardless of the other preprocessing stages. Applying windowing allows the model to iterate over sections of the data and learn patterns that way. This is an important component of LSTM as well as audio processing as each note does contain feature information, the important learning and processing comes from patterns within the music, which is where windowing helps."
      ],
      "metadata": {
        "id": "6ll7ZcWFsigz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_windows(sequence, window_size, step_size=1):\n",
        "    \"\"\"Splits a sequence into overlapping windows.\"\"\"\n",
        "    windows = []\n",
        "    for start in range(0, len(sequence) - window_size + 1, step_size):\n",
        "        end = start + window_size\n",
        "        window = sequence[start:end]\n",
        "        windows.append(window)\n",
        "    return windows\n",
        "\n",
        "\n",
        "# Apply windowing to all sequences\n",
        "window_size = 50\n",
        "step_size = 20\n",
        "\n",
        "# Function to apply windowing to an entire dataset\n",
        "def apply_windowing(sequences, window_size, step_size=1):\n",
        "    windowed_sequences = []\n",
        "    for sequence in sequences:\n",
        "        windows = create_windows(sequence, window_size, step_size)\n",
        "        windowed_sequences.extend(windows)  # Add all windows of a sequence\n",
        "    return windowed_sequences\n",
        "\n",
        "# Apply windowing to each composer's dataset\n",
        "windowed_bach = apply_windowing(augmented_combined_bach, window_size, step_size)\n",
        "windowed_mozart = apply_windowing(augmented_combined_mozart, window_size, step_size)\n",
        "windowed_beethoven = apply_windowing(augmented_combined_beethoven, window_size, step_size)\n",
        "windowed_chopin = apply_windowing(augmented_combined_chopin, window_size, step_size)\n",
        "\n",
        "# Combine windowed data\n",
        "all_windowed_data = windowed_bach + windowed_mozart + windowed_beethoven + windowed_chopin\n",
        "\n",
        "# Update labels for each windowed sequence\n",
        "windowed_labels = [0] * len(windowed_bach) + [1] * len(windowed_mozart) + \\\n",
        "                  [2] * len(windowed_beethoven) + [3] * len(windowed_chopin)"
      ],
      "metadata": {
        "id": "NVOo9Ok4q-o-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Processing Steps\n",
        "The final lines of code combine the data and ensure the size and shape match what is expected for the model. This includes flattening, tokenizing, padding, one hot encoding the labels, and splitting the data into training and testing sets."
      ],
      "metadata": {
        "id": "EewNxiess5Tg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten windowed data\n",
        "flattened_data = [note[0] for sequence in all_windowed_data for note in sequence]\n",
        "\n",
        "# Tokenize the note pitches\n",
        "all_pitches = sorted(set(flattened_data))\n",
        "note_to_int = {note: number for number, note in enumerate(all_pitches)}\n",
        "\n",
        "# Convert note sequences to integer sequences\n",
        "tokenized_data = [[note_to_int[note[0]] for note in sequence] for sequence in all_windowed_data]\n",
        "\n",
        "# Pad sequences to ensure consistent input shape\n",
        "X = pad_sequences(tokenized_data, maxlen=window_size, padding='post')\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "labels = to_categorical(windowed_labels)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "d4rnTs5WrE5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM Model\n",
        "This next section builds, compiles and trains the LSTM model.  Due to long training times, as well as to prevent overfitting, applying early stopping is included in this section. There is also adding checkpoints and saving the model for further analysis, as well as to allow for repeatability for anyone picking up this notebook. The LSTM model is composed of 8 layers, including dropout and dense layers to prevent overfitting. The use of Relu and softmax are included to help increase the gradient. Since there are 4 possible composers, the categorical_crossentropy loss function is applied and the adam optimizer is used. Using 10 epochs tended to give high accuracy, 98%, so to reduce overfitting, no more epochs are needed. The loss and accuracy funtions are printed at the end for initial analysis."
      ],
      "metadata": {
        "id": "-NCLdfGgqLnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define LSTM model\n",
        "model_windowed = Sequential()\n",
        "model_windowed.add(Embedding(len(all_pitches), 128, input_length=window_size))\n",
        "model_windowed.add(LSTM(256, return_sequences=True))\n",
        "model_windowed.add(Dropout(0.3))\n",
        "model_windowed.add(LSTM(128))\n",
        "model_windowed.add(Dropout(0.3))\n",
        "model_windowed.add(Dense(64, activation='relu'))\n",
        "model_windowed.add(Dense(4, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model_windowed.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Early Stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Create tf.data.Dataset for training\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "train_dataset = train_dataset.batch(512).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Create tf.data.Dataset for testing\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "test_dataset = test_dataset.batch(512).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath='/content/drive/MyDrive/windowed_model.keras',\n",
        "    save_weights_only=False,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True\n",
        ")\n",
        "\n",
        "# Train the model using the dataset\n",
        "model_windowed.fit(train_dataset, epochs=10, validation_data=test_dataset, callbacks=[early_stopping])\n",
        "\n",
        "model_windowed.save('/content/drive/MyDrive/windowed_model')\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model_windowed.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "4mOLYyU9qLdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_DWdfvcJqcVS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}