{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMFMmcfCEWq0qKEtBm1XbLV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kviercz/AAI-511_Group-Project/blob/main/Base_composers_output.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHsax-terpuM",
        "outputId": "f5d6260d-b497-4f2f-f5f4-6d248d936da8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "# Run this\n",
        "drive.mount('/content/drive/')\n",
        "dataset_path = '/content/drive/MyDrive/DataFiles/'\n",
        "os.chdir(dataset_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnhOxoYsfgc0",
        "outputId": "a49f9352-7fb9-4815-f026-88571cc25bfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pretty_midi\n",
            "  Downloading pretty_midi-0.2.10.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting mido\n",
            "  Downloading mido-1.3.2-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from pretty_midi) (1.25.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pretty_midi) (1.16.0)\n",
            "Collecting packaging~=23.1 (from mido)\n",
            "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Downloading mido-1.3.2-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pretty_midi\n",
            "  Building wheel for pretty_midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretty_midi: filename=pretty_midi-0.2.10-py3-none-any.whl size=5592288 sha256=115e31029828e3045198f22c54676d7b1866792647aede169cfabbdc185f06c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/a5/30/7b8b7f58709f5150f67f98fde4b891ebf0be9ef07a8af49f25\n",
            "Successfully built pretty_midi\n",
            "Installing collected packages: packaging, mido, pretty_midi\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.1\n",
            "    Uninstalling packaging-24.1:\n",
            "      Successfully uninstalled packaging-24.1\n",
            "Successfully installed mido-1.3.2 packaging-23.2 pretty_midi-0.2.10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping file Anhang 14-3.mid due to KeySignatureError: Could not decode key with 3 flats and mode 255\n"
          ]
        }
      ],
      "source": [
        "!pip install pretty_midi mido\n",
        "\n",
        "import pretty_midi\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding, Bidirectional\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from mido import KeySignatureError\n",
        "import os\n",
        "import random\n",
        "\n",
        "# --- Data Loading and Preprocessing ---\n",
        "\n",
        "def load_midi_features(directory):\n",
        "    \"\"\"Loads MIDI files and extracts pitch, duration, and velocity features.\"\"\"\n",
        "    pitch_sequences = []\n",
        "    duration_sequences = []\n",
        "    velocity_sequences = []\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".mid\"):\n",
        "            midi_path = os.path.join(directory, filename)\n",
        "            try:\n",
        "                midi = pretty_midi.PrettyMIDI(midi_path)\n",
        "                for instrument in midi.instruments:\n",
        "                    pitches = [note.pitch for note in instrument.notes]\n",
        "                    durations = [note.end - note.start for note in instrument.notes]\n",
        "                    velocities = [note.velocity for note in instrument.notes]\n",
        "                    pitch_sequences.append(pitches)\n",
        "                    duration_sequences.append(durations)\n",
        "                    velocity_sequences.append(velocities)\n",
        "            except KeySignatureError as e:\n",
        "                print(f\"Skipping file {filename} due to KeySignatureError: {e}\")\n",
        "\n",
        "    return pitch_sequences, duration_sequences, velocity_sequences\n",
        "\n",
        "# Load features for each composer\n",
        "bach_pitches, bach_durations, bach_velocities = load_midi_features(\"/content/drive/MyDrive/DataFiles/Bach/\")\n",
        "mozart_pitches, mozart_durations, mozart_velocities = load_midi_features(\"/content/drive/MyDrive/DataFiles/Mozart/\")\n",
        "beethoven_pitches, beethoven_durations, beethoven_velocities = load_midi_features(\"/content/drive/MyDrive/DataFiles/Beethoven/\")\n",
        "chopin_pitches, chopin_durations, chopin_velocities = load_midi_features(\"/content/drive/MyDrive/DataFiles/Chopin/\")\n",
        "\n",
        "def normalize_features(pitch_sequences, duration_sequences, velocity_sequences):\n",
        "    \"\"\"Normalizes pitches, durations, and velocities to a 0-1 range.\"\"\"\n",
        "    normalized_pitches = [[note / 127.0 for note in sequence] for sequence in pitch_sequences]\n",
        "    max_duration = max([max(durations) for durations in duration_sequences if durations])  # Find max duration\n",
        "    normalized_durations = [[duration / max_duration for duration in sequence] for sequence in duration_sequences]\n",
        "    normalized_velocities = [[velocity / 127.0 for velocity in sequence] for sequence in velocity_sequences]\n",
        "\n",
        "    return normalized_pitches, normalized_durations, normalized_velocities\n",
        "\n",
        "# Normalize the features\n",
        "bach_pitches, bach_durations, bach_velocities = normalize_features(bach_pitches, bach_durations, bach_velocities)\n",
        "mozart_pitches, mozart_durations, mozart_velocities = normalize_features(mozart_pitches, mozart_durations, mozart_velocities)\n",
        "beethoven_pitches, beethoven_durations, beethoven_velocities = normalize_features(beethoven_pitches, beethoven_durations, beethoven_velocities)\n",
        "chopin_pitches, chopin_durations, chopin_velocities = normalize_features(chopin_pitches, chopin_durations, chopin_velocities)\n",
        "\n",
        "\n",
        "\n",
        "# --- Data Balancing ---\n",
        "\n",
        "# min_samples = min(len(bach_data), len(mozart_data), len(beethoven_data), len(chopin_data))\n",
        "\n",
        "# balanced_bach = random.sample(bach_data, min_samples)\n",
        "# balanced_mozart = random.sample(mozart_data, min_samples)\n",
        "# balanced_beethoven = random.sample(beethoven_data, min_samples)\n",
        "# balanced_chopin = random.sample(chopin_data, min_samples)\n",
        "\n",
        "# --- Data Augmentation ---\n",
        "\n",
        "def augment_sequence(sequence, num_augmentations=2):\n",
        "    \"\"\"Augments a note sequence by transposing and creating variations.\"\"\"\n",
        "    augmented_sequences = [sequence]\n",
        "    for _ in range(num_augmentations):\n",
        "        transpose_amount = random.randint(-5, 5)  # Transpose by up to 5 semitones\n",
        "        augmented_sequence = [[note[0] + transpose_amount, note[1], note[2]] for note in sequence] # Transpose only the pitch value (index 0)\n",
        "        augmented_sequences.append(augmented_sequence)\n",
        "    return augmented_sequences\n",
        "\n",
        "def combine_features(pitch_sequences, duration_sequences, velocity_sequences):\n",
        "    \"\"\"Combines pitch, duration, and velocity features into a single array.\"\"\"\n",
        "    combined_features = []\n",
        "    for pitches, durations, velocities in zip(pitch_sequences, duration_sequences, velocity_sequences):\n",
        "        sequence_length = min(len(pitches), len(durations), len(velocities))\n",
        "        combined_sequence = []\n",
        "        for i in range(sequence_length):\n",
        "            combined_sequence.append([pitches[i], durations[i], velocities[i]])\n",
        "        combined_features.append(combined_sequence)\n",
        "    return combined_features\n",
        "\n",
        "# Combine features for each composer\n",
        "combined_bach = combine_features(bach_pitches, bach_durations, bach_velocities)\n",
        "combined_mozart = combine_features(mozart_pitches, mozart_durations, mozart_velocities)\n",
        "combined_beethoven = combine_features(beethoven_pitches, beethoven_durations, beethoven_velocities)\n",
        "combined_chopin = combine_features(chopin_pitches, chopin_durations, chopin_velocities)\n",
        "\n",
        "# Augment combined data\n",
        "augmented_combined_bach = [seq for orig_seq in combined_bach for seq in augment_sequence(orig_seq)]\n",
        "augmented_combined_mozart = [seq for orig_seq in combined_mozart for seq in augment_sequence(orig_seq)]\n",
        "augmented_combined_beethoven = [seq for orig_seq in combined_beethoven for seq in augment_sequence(orig_seq)]\n",
        "augmented_combined_chopin = [seq for orig_seq in combined_chopin for seq in augment_sequence(orig_seq)]\n",
        "\n",
        "def augment_with_noise(sequence, noise_factor=0.01):\n",
        "    \"\"\"Adds noise to the sequence to create variation.\"\"\"\n",
        "    noisy_sequence = []\n",
        "    for note in sequence:\n",
        "        noisy_note = [\n",
        "            note[0] + random.uniform(-noise_factor, noise_factor),  # Pitch noise\n",
        "            note[1] + random.uniform(-noise_factor, noise_factor),  # Duration noise\n",
        "            note[2] + random.uniform(-noise_factor, noise_factor)   # Velocity noise\n",
        "        ]\n",
        "        noisy_sequence.append(noisy_note)\n",
        "    return noisy_sequence\n",
        "\n",
        "def augment_with_tempo(sequence, tempo_factor_range=(0.9, 1.1)):\n",
        "    \"\"\"Scales the tempo of the sequence.\"\"\"\n",
        "    tempo_factor = random.uniform(*tempo_factor_range)\n",
        "    tempo_scaled_sequence = [[note[0], note[1] * tempo_factor, note[2]] for note in sequence]\n",
        "    return tempo_scaled_sequence\n",
        "\n",
        "# Augment with noise and tempo scaling\n",
        "augmented_combined_bach.extend([augment_with_noise(seq) for seq in augmented_combined_bach])\n",
        "augmented_combined_bach.extend([augment_with_tempo(seq) for seq in augmented_combined_bach])\n",
        "\n",
        "augmented_combined_mozart.extend([augment_with_noise(seq) for seq in augmented_combined_mozart])\n",
        "augmented_combined_mozart.extend([augment_with_tempo(seq) for seq in augmented_combined_mozart])\n",
        "\n",
        "augmented_combined_beethoven.extend([augment_with_noise(seq) for seq in augmented_combined_beethoven])\n",
        "augmented_combined_beethoven.extend([augment_with_tempo(seq) for seq in augmented_combined_beethoven])\n",
        "\n",
        "augmented_combined_chopin.extend([augment_with_noise(seq) for seq in augmented_combined_chopin])\n",
        "augmented_combined_chopin.extend([augment_with_tempo(seq) for seq in augmented_combined_chopin])\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def create_windows(sequence, window_size, step_size=1):\n",
        "    \"\"\"Splits a sequence into overlapping windows.\"\"\"\n",
        "    windows = []\n",
        "    for start in range(0, len(sequence) - window_size + 1, step_size):\n",
        "        end = start + window_size\n",
        "        window = sequence[start:end]\n",
        "        windows.append(window)\n",
        "    return windows\n",
        "\n",
        "\n",
        "# Apply windowing to all sequences\n",
        "window_size = 50  # Choose an appropriate window size\n",
        "step_size = 10    # Overlapping windows (can be set to 1 for fully overlapping)\n",
        "\n",
        "# Function to apply windowing to an entire dataset\n",
        "def apply_windowing(sequences, window_size, step_size=1):\n",
        "    windowed_sequences = []\n",
        "    for sequence in sequences:\n",
        "        windows = create_windows(sequence, window_size, step_size)\n",
        "        windowed_sequences.extend(windows)  # Add all windows of a sequence\n",
        "    return windowed_sequences\n",
        "\n",
        "# Apply windowing to each composer's dataset\n",
        "windowed_bach = apply_windowing(augmented_combined_bach, window_size, step_size)\n",
        "windowed_mozart = apply_windowing(augmented_combined_mozart, window_size, step_size)\n",
        "windowed_beethoven = apply_windowing(augmented_combined_beethoven, window_size, step_size)\n",
        "windowed_chopin = apply_windowing(augmented_combined_chopin, window_size, step_size)\n",
        "\n",
        "# Combine windowed data\n",
        "all_windowed_data = windowed_bach + windowed_mozart + windowed_beethoven + windowed_chopin\n",
        "\n",
        "# Flatten windowed data to extract unique pitches\n",
        "# flattened_data = [note for sequence in all_windowed_data for note in sequence]\n",
        "\n",
        "# Update labels for each windowed sequence\n",
        "windowed_labels = [0] * len(windowed_bach) + [1] * len(windowed_mozart) + \\\n",
        "                  [2] * len(windowed_beethoven) + [3] * len(windowed_chopin)\n",
        "\n",
        "# Flatten windowed data to extract unique pitches\n",
        "flattened_data = [note[0] for sequence in all_windowed_data for note in sequence] # Extract only the pitch value\n",
        "\n",
        "# Tokenize the note pitches\n",
        "all_pitches = sorted(set(flattened_data)) # Create a set from the flattened pitches\n",
        "note_to_int = {note: number for number, note in enumerate(all_pitches)}\n",
        "\n",
        "# Convert note sequences to integer sequences (using only pitch)\n",
        "tokenized_data = [[note_to_int[note[0]] for note in sequence] for sequence in all_windowed_data]\n",
        "# Pad sequences to ensure consistent input shape\n",
        "X = pad_sequences(tokenized_data, maxlen=window_size, padding='post')\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "labels = to_categorical(windowed_labels)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Combine augmented data\n",
        "# all_combined_data = augmented_combined_bach + augmented_combined_mozart + augmented_combined_beethoven + augmented_combined_chopin\n",
        "\n",
        "\n",
        "# Combine all data\n",
        "# all_combined_data = augmented_combined_bach + augmented_combined_mozart + augmented_combined_beethoven + augmented_combined_chopin\n",
        "\n",
        "# # Combine augmented data and create labels\n",
        "# all_data = augmented_bach + augmented_mozart + augmented_beethoven + augmented_chopin\n",
        "# labels = [0] * len(augmented_combined_bach) + [1] * len(augmented_combined_mozart) + \\\n",
        "        #  [2] * len(augmented_combined_beethoven) + [3] * len(augmented_combined_chopin)\n",
        "\n",
        "# # --- Data Preparation ---\n",
        "\n",
        "# # Tokenize the note pitches\n",
        "# all_pitches = sorted(set(note for sequence in all_data for note in sequence))\n",
        "# note_to_int = {note: number for number, note in enumerate(all_pitches)}\n",
        "# int_to_note = {number: note for note, number in note_to_int.items()}\n",
        "\n",
        "# Convert note sequences to integer sequences\n",
        "# tokenized_data = [[note_to_int[note] for note in sequence] for sequence in all_data]\n",
        "\n",
        "# Pad sequences\n",
        "# Update max sequence length for the new data\n",
        "# max_combined_sequence_length = max([len(seq) for seq in all_combined_data])\n",
        "\n",
        "# Pad combined sequences\n",
        "# X_combined = pad_sequences(all_combined_data, maxlen=max_combined_sequence_length, padding='post', dtype='float32')\n",
        "\n",
        "# Split data into training and testing sets\n",
        "# X_train_combined, X_test_combined, y_train, y_test = train_test_split(X_combined, labels, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "y_train = to_categorical(y_train, num_classes=4)\n",
        "y_test = to_categorical(y_test, num_classes=4)"
      ],
      "metadata": {
        "id": "h_dqz0sbJ6rD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import BatchNormalization\n",
        "\n",
        "# Create LSTM model\n",
        "model_combined = Sequential()\n",
        "model_combined.add(LSTM(256, return_sequences=True, input_shape=(window_size, 3)))  # 3 features\n",
        "model_combined.add(BatchNormalization())\n",
        "model_combined.add(Dropout(0.3))\n",
        "model_combined.add(LSTM(128))\n",
        "model_combined.add(BatchNormalization())\n",
        "model_combined.add(Dropout(0.3))\n",
        "model_combined.add(Dense(64, activation='relu'))\n",
        "model_combined.add(Dense(4, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model_combined.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model_combined.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model_combined.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQ5dm8meRsvM",
        "outputId": "d5c8d1b4-16ba-4835-90e8-16699b6b31a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "445/659 [===================>..........] - ETA: 2:06:45 - loss: 1.3843 - accuracy: 0.3098"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- LSTM Model for Windowed Input ---\n",
        "\n",
        "# Define LSTM model\n",
        "model_windowed = Sequential()\n",
        "model_windowed.add(Embedding(len(all_pitches), 128, input_length=window_size))  # Window size as input length\n",
        "model_windowed.add(LSTM(256, return_sequences=True))\n",
        "model_windowed.add(Dropout(0.3))\n",
        "model_windowed.add(LSTM(128))\n",
        "model_windowed.add(Dropout(0.3))\n",
        "model_windowed.add(Dense(64, activation='relu'))\n",
        "model_windowed.add(Dense(4, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model_windowed.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Early Stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "model_windowed.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model_windowed.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqnlkPYPPRAf",
        "outputId": "2d965d91-6ef7-4a81-c7f4-d8402885c938"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "10991/23861 [============>.................] - ETA: 28:00:22 - loss: 0.6643 - accuracy: 0.7198"
          ]
        }
      ]
    }
  ]
}